# CSS Selector Discovery System

AI-powered CSS selector discovery for web scraping. Give it a URL, and it automatically finds the best CSS selectors for extracting headlines, authors, dates, body text, and related content.

## üéØ How It Works

1. **Fetch HTML** - Downloads raw HTML from the URL
2. **AI Analysis** - AI reads the HTML and finds CSS selectors
3. **Validation** - Tests each selector to ensure it works
4. **Storage** - Saves validated selectors to JSON files

## üì¶ Project Structure

```
.
‚îú‚îÄ‚îÄ main.py                   # Entry point / orchestrator
‚îú‚îÄ‚îÄ selector_discovery.py     # AI reads HTML and finds selectors
‚îú‚îÄ‚îÄ selector_validator.py     # Validates selectors work on pages
‚îú‚îÄ‚îÄ selector_storage.py       # Handles JSON file operations
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ .env                      # API keys (create this)
‚îî‚îÄ‚îÄ selectors/               # Output directory
    ‚îî‚îÄ‚îÄ selectors_*.json      # Selector files per domain
```

## üöÄ Quick Start

### 1. Install Dependencies

```bash
uv pip install -r requirements.txt
```

### 2. Set Up API Key

Create a `.env` file:

```bash
GEMINI_KEY=your_gemini_api_key_here
```

### 3. Run It

```bash
# Process default test URLs
uv run main.py

# Process a single URL
uv run main.py --url https://example.com/article

# Process URLs from a file
uv run main.py --file urls.txt

# Force re-discovery
uv run main.py --url https://example.com --force

# Show summary of saved selectors
uv run main.py --summary
```

## üìÑ Output Format

Selectors are saved as JSON files in the `selectors/` directory:

```json
{
  "domain": "example.com",
  "source_url": "https://example.com/article",
  "discovered_at": "2024-12-22T15:30:00",
  "version": "1.0",
  "selectors": {
    "headline": {
      "primary": "h1.article-title",
      "fallback": "h1",
      "tertiary": "h2",
      "working_priority": "primary",
      "tested": true
    },
    "author": {
      "primary": "a[href*='/author/']",
      "fallback": ".byline",
      "tertiary": "NA",
      "working_priority": "primary",
      "tested": true
    },
    "date": {
      "primary": "time.published-date",
      "fallback": "time",
      "tertiary": ".date",
      "working_priority": "primary",
      "tested": true
    },
    "body_text": {
      "primary": "article.content p",
      "fallback": "article p",
      "tertiary": "p",
      "working_priority": "primary",
      "tested": true
    },
    "related_content": {
      "primary": "aside.related a",
      "fallback": ".sidebar a",
      "tertiary": "NA",
      "working_priority": "fallback",
      "tested": true
    }
  }
}
```

## üîß Usage Examples

### Basic Usage

```python
from main import SelectorDiscoveryPipeline
import os

# Initialize
pipeline = SelectorDiscoveryPipeline(os.getenv('GEMINI_KEY'))

# Process a URL
success = pipeline.process_url('https://example.com/article')

# Show summary
pipeline.show_summary()
```

### Using Discovered Selectors

```python
from selector_storage import SelectorStorage
from bs4 import BeautifulSoup
import requests

# Load selectors
storage = SelectorStorage()
selectors = storage.load_selectors('example.com')

# Scrape using selectors (fast & free!)
url = 'https://example.com/another-article'
html = requests.get(url).text
soup = BeautifulSoup(html, 'html.parser')

# Extract data
headline_selector = selectors['headline']['primary']
headline = soup.select_one(headline_selector).get_text()

print(f"Headline: {headline}")
```

## üìä Performance

| Approach | Time | Cost | Success Rate |
|----------|------|------|--------------|
| **AI Discovery (once)** | ~3 sec | $0.001 | ~90% |
| **BeautifulSoup (production)** | ~0.5 sec | FREE | 100% |

**For 1000 articles:**
- Discovery: 3 seconds, $0.001
- Scraping: 500 seconds (8 min), $0
- **Total: ~8 minutes, $0.001**

Compare to using AI every time: 2.2 hours, $1.00

## üé® Features

‚úÖ **AI-Powered** - Uses Gemini to read HTML and find selectors
‚úÖ **Fast** - 3 seconds per domain (one-time discovery)
‚úÖ **Cheap** - $0.001 per domain
‚úÖ **Validated** - Tests each selector before saving
‚úÖ **Organized** - Clean JSON output per domain
‚úÖ **Fallback** - Uses heuristics when AI fails
‚úÖ **CLI** - Command-line interface included

## üõ†Ô∏è Modules

### selector_discovery.py
AI-powered selector discovery. Extracts clean HTML and asks AI to find CSS selectors.

**Key Methods:**
- `discover_from_html(url, html)` - Main discovery method
- `_extract_content_html(html)` - Extracts main content area
- `_get_selectors_from_ai(url, html)` - Asks AI for selectors

### selector_validator.py
Validates selectors work on actual pages.

**Key Methods:**
- `validate_selectors(url, selectors)` - Test all selectors
- `quick_test(url, selector)` - Quick test for single selector
- `_test_selector(soup, field, selector)` - Test individual selector

### selector_storage.py
Handles JSON file storage and retrieval.

**Key Methods:**
- `save_selectors(url, selectors)` - Save to JSON
- `load_selectors(domain)` - Load from JSON
- `get_summary()` - Get overview of all saved selectors
- `export_summary()` - Export summary to file

### main.py
Orchestrates the entire pipeline.

**Key Methods:**
- `process_url(url, force)` - Process single URL
- `process_urls(urls, force)` - Process multiple URLs
- `show_summary()` - Display summary

## üîç How It Works (Technical)

### Phase 1: Content Extraction
```
Full HTML (2MB) ‚Üí Remove noise (scripts/styles) ‚Üí
Find main content (<article>, <main>) ‚Üí
Extract ~30k chars ‚Üí Send to AI
```

### Phase 2: AI Analysis
```
AI reads HTML ‚Üí Finds actual class names ‚Üí
Returns 3 selectors per field (primary/fallback/tertiary) ‚Üí
Smart fallback if AI fails
```

### Phase 3: Validation
```
Test each selector on actual page ‚Üí
Find first working selector ‚Üí
Mark which priority level worked ‚Üí
Save validated selectors
```

## üö´ Skipping Paywalled Sites

```python
PAYWALLED_SITES = [
    'wsj.com',
    'nytimes.com',
    'washingtonpost.com',
]

# Skip paywalled sites
if any(site in url for site in PAYWALLED_SITES):
    print(f"Skipping paywalled site: {url}")
    continue
```

## üìù URLs File Format

Create `urls.txt` with one URL per line:

```
https://example.com/article1
https://example.com/article2
# Comments are allowed
https://example.com/article3
```

## üêõ Troubleshooting

**AI returns all NA:**
- Site might not have semantic HTML
- Fallback heuristics will be used automatically

**Selectors don't work:**
- Site might use JavaScript rendering (content not in HTML)
- Consider using Selenium for JavaScript-heavy sites

**API Key Error:**
- Check `.env` file exists
- Verify `GEMINI_KEY` is set correctly
